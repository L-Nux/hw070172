{
    "abbott_venice_nodate": {
        "author": "Abbott, Alison",
        "file": "Abbott - VENICE GETS A TIME MACHINE.pdf:files/16/Abbott - VENICE GETS A TIME MACHINE.pdf:application/pdf",
        "language": "en",
        "pages": "4",
        "rCite": "abbott_venice_nodate",
        "rType": "article",
        "title": "{VENICE {GETS {A {TIME {MACHINE"
    },
    "aurast_big_2016": {
        "abstract": "With networked large data collections and suitable quantitative methods new research questions become possible. Within the DARIAH-DE cluster 5 new philologico-critical and historical comparing perspectives enter the modelling of large data collections. Consequently new, tailored analytical tools can be developed. This includes the chance to test in empirical ways theoretical presuppositions and the extension and advancement of the modes of the production of scientific knowledge. The article describes two analytical tools developed within cluster 5: the Dariah-DKPro-Wrapper, a software package for the automated linguistical analysis, and Cosmotool, a search- and analysis tool for the exploration of transboundary lives by means of structured and unstructured data collections.",
        "author": "Aurast, Anna and Gradl, Tobias and Pernes, Stefan and Pielström, Steffen",
        "doi": "DOI: https://doi.org/10.1515/bfp-2016-0033",
        "file": "Full Text:files/351/Aurast et al. - 2016 - Big Data und Smart Data in den Geisteswissenschaft.pdf:application/pdf",
        "journal": "Bibliothek Forschung und Praxis",
        "keywords": "obj\\_Tools, goal\\_Dissemination, goal\\_Analysis, act\\_ContentAnalysis, obj\\_Data, act\\_RelationalAnalysis, meta\\_Teaching, act\\_Collaborating",
        "number": "2",
        "rCite": "aurast_big_2016",
        "rType": "article",
        "title": "Big {Data und {Smart {Data in den {Geisteswissenschaften",
        "volume": "40",
        "year": "2016"
    },
    "bahdanau_neural_2016": {
        "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
        "annote": "Comment: Accepted at ICLR 2015 as oral presentation",
        "author": "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
        "file": "Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:files/56/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf",
        "journal": "arXiv:1409.0473 [cs, stat]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing",
        "language": "en",
        "month": "may,",
        "note": "arXiv: 1409.0473",
        "rCite": "bahdanau_neural_2016",
        "rType": "article",
        "title": "Neural {Machine {Translation by {Jointly {Learning to {Align and {Translate",
        "url": "http://arxiv.org/abs/1409.0473",
        "urldate": "2020-10-10",
        "year": "2016"
    },
    "baroni_new_2006": {
        "abstract": "In this article we describe an approach to the identification of ‘translationese’ based on monolingual comparable corpora and machine learning techniques for text categorization. The article reports on experiments in which support vector machines (SVMs) are employed to recognize translated text in a corpus of Italian articles from the geopolitical domain. An ensemble of SVMs reaches 86.7\\% accuracy with 89.3\\% precision and 83.3\\% recall on this task. A preliminary analysis of the features used by the SVMs suggests that the distribution of function words and morphosyntactic categories in general, and personal pronouns and adverbs in particular, are among the cues used by the SVMs to perform the discrimination task. A follow-up experiment shows that the performance attained by SVMs is well above the average performance of ten human subjects, including five professional translators, on the same task. Our results offer solid evidence supporting the translationese hypothesis, and our method seems to have promising applications in translation studies and in quantitative style analysis in general. Implications for the machine learning/text categorization community are equally important, both because this is a novel application and especially because we provide explicit evidence that a relatively knowledge-poor machine learning algorithm can outperform human beings in a text classification task.",
        "author": "Baroni, Marco and Bernardini, Silvia",
        "doi": "10.1093/llc/fqi039",
        "file": "Submitted Version:files/209/Baroni and Bernardini - 2006 - A New Approach to the Study of Translationese Mac.pdf:application/pdf",
        "issn": "0268-1145, 1477-4615",
        "journal": "Literary and Linguistic Computing",
        "keywords": "bigdata, t\\_MachineLearning",
        "language": "en",
        "month": "sep,",
        "number": "3",
        "pages": "259--274",
        "rCite": "baroni_new_2006",
        "rType": "article",
        "shorttitle": "A {New {Approach to the {Study of {Translationese",
        "title": "A {New {Approach to the {Study of {Translationese: {Machine-learning the {Difference between {Original and {Translated {Text",
        "url": "http://llc.oxfordjournals.org/content/21/3/259",
        "urldate": "2013-03-19",
        "volume": "21",
        "year": "2006"
    },
    "barranha_derivative_2018": {
        "author": "Barranha, Helena",
        "doi": "10.1111/muse.12190",
        "file": "Barranha - 2018 - Derivative Narratives The Multiple Lives of a Mas.pdf:files/18/Barranha - 2018 - Derivative Narratives The Multiple Lives of a Mas.pdf:application/pdf",
        "issn": "1350-0775, 1468-0033",
        "journal": "Museum International",
        "language": "en",
        "month": "jan,",
        "number": "1-2",
        "pages": "22--33",
        "rCite": "barranha_derivative_2018",
        "rType": "article",
        "shorttitle": "Derivative {Narratives",
        "title": "Derivative {Narratives: {The {Multiple {Lives of a {Masterpiece on the {Internet",
        "url": "https://www.tandfonline.com/doi/full/10.1111/muse.12190",
        "urldate": "2020-10-10",
        "volume": "70",
        "year": "2018"
    },
    "beer_interdisciplinary_2014": {
        "abstract": "The exchange and reusability of data used for research in the humanities is one of the goals of DARIAH. To increase the interoperability of data set between disciplines we present an overview and recommendations of measures to achieve this. We account for the finding and fetching of data with legal aspects in mind. This is achieved through standardized methods of discovery and transfer via interfaces on the web. Furthermore, we consider syntactic and semantic interoperability of data for use in different fields of study. Standardized metadata sets are one way to achieve this and we present some of them in this paper. The importance for scholars to find and be able to process data that is be relevant to their work is the main motivation of this document and for the aspects of the digital humanities covered within. We present options for each of the four aspects that we identified (APIs and Protocols, Standards, Identifiers and Licensing).",
        "address": "Göttingen",
        "author": "Beer, Nikolaos and Herold, Kristin and Kolbmann, Wibke and Kollatz, Thomas and Romanello, Matteo and Rose, Sebastian and Walkowski, Niels-Oliver",
        "institution": "DARIAH-DE",
        "keywords": "obj\\_DigitalHumanities, obj\\_Humanities",
        "language": "en",
        "note": "00002",
        "number": "3",
        "pages": "46",
        "rCite": "beer_interdisciplinary_2014",
        "rType": "techreport",
        "title": "Interdisciplinary {Interoperability",
        "url": "http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-3.pdf",
        "year": "2014"
    },
    "bender_climbing_2020": {
        "abstract": "The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.",
        "address": "Online",
        "author": "Bender, Emily M. and Koller, Alexander",
        "booktitle": "Proceedings of the 58th {Annual {Meeting of the {Association for {Computational {Linguistics",
        "doi": "10.18653/v1/2020.acl-main.463",
        "file": "Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:files/68/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:application/pdf",
        "language": "en",
        "pages": "5185--5198",
        "publisher": "Association for Computational Linguistics",
        "rCite": "bender_climbing_2020",
        "rType": "inproceedings",
        "shorttitle": "Climbing towards {NLU",
        "title": "Climbing towards {NLU: {On {Meaning {Form and {Understanding in the {Age of {Data",
        "url": "https://www.aclweb.org/anthology/2020.acl-main.463",
        "urldate": "2020-10-10",
        "year": "2020"
    },
    "best_surface_nodate": {
        "abstract": "In the text-based disciplines, psychoanalysis and Marxism have had a major influence on how we read, and this has been expressed most consistently in the practice of symptomatic reading, a mode of interpretation that assumes that a text's truest meaning lies in what it does not say, describes textual surfaces as superfluous, and seeks to unmask hidden meanings. For symptomatic readers, texts possess meanings that are veiled, latent, all but absent if it were not for their irrepressible and recurring symptoms. Noting the recent trend away from ideological demystification, this essay proposes various modes of \"surface reading\" that together strive to accurately depict the truth to which a text bears witness. Surface reading broadens the scope of critique to include the kinds of interpretive activity that seek to understand the complexity of literary surfaces---surfaces that have been rendered invisible by symptomatic reading.",
        "author": "Best, Stephen and Marcus, Sharon",
        "doi": "DOI:10.1525/rep.2009.108.1.1",
        "file": "Full Text:files/322/Best and Marcus - Surface Reading An Introduction.pdf:application/pdf",
        "keywords": "obj\\_Text, act\\_ContentAnalysis, act\\_RelationalAnalysis, act\\_Theorizing, act\\_Query/Retrieve",
        "number": "1 (Fall 2009)",
        "pages": "1--21",
        "rCite": "best_surface_nodate",
        "rType": "article",
        "series": "Representations, {University of {California {Press",
        "title": "Surface {Reading: {An {Introduction",
        "url": "http://www.jstor.org/stable/10.1525/rep.2009.108.1.1",
        "volume": "108"
    },
    "bhattacharyya_deep_2020": {
        "address": "Boston",
        "editor": "Bhattacharyya, Siddhartha and Sasel, Vaclav and Hassanien, Aboul Ella and Saha, Satadal and Tripathy, B. K.",
        "file": "Bhattacharyya et al. - 2020 - Deep learning.pdf:files/81/Bhattacharyya et al. - 2020 - Deep learning.pdf:application/pdf",
        "isbn": "978-3-11-067079-0",
        "language": "en",
        "publisher": "DE GRUYTER",
        "rCite": "bhattacharyya_deep_2020",
        "rType": "book",
        "title": "Deep learning",
        "year": "2020"
    },
    "bojanowski_enriching_2017": {
        "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
        "annote": "Comment: Accepted to TACL. The two first authors contributed equally",
        "author": "Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas",
        "file": "Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:files/34/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf",
        "journal": "arXiv:1607.04606 [cs]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning",
        "language": "en",
        "month": "jun,",
        "note": "arXiv: 1607.04606",
        "rCite": "bojanowski_enriching_2017",
        "rType": "article",
        "title": "Enriching {Word {Vectors with {Subword {Information",
        "url": "http://arxiv.org/abs/1607.04606",
        "urldate": "2020-10-10",
        "year": "2017"
    },
    "bolukbasi_man_2016": {
        "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is ﬁrst shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender deﬁnition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We deﬁne metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to “debias” the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms signiﬁcantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
        "author": "Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam",
        "file": "Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:files/71/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homem.pdf:application/pdf",
        "journal": "arXiv:1607.06520 [cs, stat]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence",
        "language": "en",
        "month": "jul,",
        "note": "arXiv: 1607.06520",
        "rCite": "bolukbasi_man_2016",
        "rType": "article",
        "shorttitle": "Man is to {Computer {Programmer as {Woman is to {Homemaker?",
        "title": "Man is to {Computer {Programmer as {Woman is to {Homemaker? {Debiasing {Word {Embeddings",
        "url": "http://arxiv.org/abs/1607.06520",
        "urldate": "2020-10-10",
        "year": "2016"
    },
    "burgoyne_comparative_2007": {
        "address": "Vienna, Austria",
        "author": "Burgoyne, John Ashley and Pugin, Laurent and Eustace, Greg and Fujinaga, Ichiro",
        "booktitle": "Proceedings of the 8th {International {Conference on {Music {Information {Retrieval ({ISMIR 2007)",
        "keywords": "bigdata, act\\_DataRecognition, obj\\_SheetMusic",
        "language": "en",
        "rCite": "burgoyne_comparative_2007",
        "rType": "inproceedings",
        "title": "A {Comparative {Survey of {Image {Binarisation {Algorithms for {Optical {Recognition on {Degraded {Musical {Sources",
        "url": "http://www.aruspix.net/publications/burgoyne07comparative.pdf",
        "volume": "509-12",
        "year": "2007"
    },
    "card_structure_1997": {
        "author": "Card, Stuart K. and Mackinlay, Jock",
        "booktitle": "Proceedings of {IEEE {Symposium on {Information {Visualization 1997",
        "doi": "10.1109/INFVIS.1997.636792",
        "language": "en",
        "pages": "92--99",
        "rCite": "card_structure_1997",
        "rType": "inproceedings",
        "title": "The {Structure of the {Information {Visualization {Design {Space",
        "url": "http://www.cs.ubc.ca/~tmm/courses/old533/readings/card96structure.pdf",
        "year": "1997"
    },
    "chaney_visualizing_2012": {
        "author": "Chaney, Allison and Blei, David M.",
        "booktitle": "{AAAI {Publications {Sixth {International {AAAI {Conference on {Weblogs and {Social {Media",
        "keywords": "act\\_Visualizing, AnalyzeStatistically",
        "language": "en",
        "pages": "4",
        "rCite": "chaney_visualizing_2012",
        "rType": "inproceedings",
        "title": "Visualizing {Topic {Models",
        "url": "http://www.cs.columbia.edu/~blei/papers/ChaneyBlei2012.pdf",
        "year": "2012"
    },
    "collar_networks_2015": {
        "abstract": "The application of method and theory from network science to archaeology has dramatically increased over the last decade. In this article, we document this growth over time, discuss several of the important concepts that are used in the application of network approaches to archaeology, and introduce the other articles in this special issue on networks in archaeology. We argue that the suitability and contribution of network science techniques within particular archaeological research contexts can be usefully explored by scrutinizing the past phenomena under study, how these are abstracted into concepts, and how these in turn are represented as network data. For this reason, each of the articles in this special issue is discussed in terms of the phenomena that they seek to address, the abstraction in terms of concepts that they use to study connectivity, and the representations of network data that they employ in their analyses. The approaches currently being used are diverse and interdisciplinary, which we think are evidence of a healthy exploratory stage in the application of network science in archaeology. To facilitate further innovation, application, and collaboration, we also provide a glossary of terms that are currently being used in network science and especially those in the applications to archaeological case studies.",
        "author": "Collar, Anna and Coward, Fiona and Brughmans, Tom and Mills, Barbara J.",
        "doi": "10.1007/s10816-014-9235-6",
        "file": "Accepted Version:files/321/Collar et al. - 2015 - Networks in Archaeology Phenomena, Abstraction, R.pdf:application/pdf",
        "issn": "1072-5369, 1573-7764",
        "journal": "Journal of Archaeological Method and Theory",
        "keywords": "act\\_NetworkAnalysis, act\\_Theorizing, obj\\_ANTHROPOLOGY, obj\\_Archaeology, obj\\_Network science, obj\\_Relational archaeology",
        "language": "en",
        "month": "jan,",
        "number": "1",
        "pages": "1--32",
        "rCite": "collar_networks_2015",
        "rType": "article",
        "shorttitle": "Networks in {Archaeology",
        "title": "Networks in {Archaeology: {Phenomena {Abstraction {Representation",
        "url": "http://link.springer.com/article/10.1007/s10816-014-9235-6",
        "urldate": "2015-08-05",
        "volume": "22",
        "year": "2015"
    },
    "conneau_supervised_2017": {
        "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.",
        "address": "Copenhagen, Denmark",
        "author": "Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loïc and Bordes, Antoine",
        "booktitle": "Proceedings of the 2017 {Conference on {Empirical {Methods in {Natural           {Language {Processing",
        "doi": "10.18653/v1/D17-1070",
        "file": "Conneau et al. - 2017 - Supervised Learning of Universal Sentence Represen.pdf:files/85/Conneau et al. - 2017 - Supervised Learning of Universal Sentence Represen.pdf:application/pdf",
        "language": "en",
        "pages": "670--680",
        "publisher": "Association for Computational Linguistics",
        "rCite": "conneau_supervised_2017",
        "rType": "inproceedings",
        "title": "Supervised {Learning of {Universal {Sentence {Representations from {Natural {Language {Inference {Data",
        "url": "http://aclweb.org/anthology/D17-1070",
        "urldate": "2020-10-10",
        "year": "2017"
    },
    "czmiel_adaquate_2003": {
        "author": "Czmiel, Alexander",
        "keywords": "t\\_Encoding, t\\_XML, goal\\_Enrichment",
        "language": "de",
        "month": "oct,",
        "rCite": "czmiel_adaquate_2003",
        "rType": "phdthesis",
        "school": "Köln",
        "title": "Adäquate {Markupsysteme für die digitale {Behandlung altägyptischer {Texte",
        "type": "Thesis ({MA level)",
        "url": "http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf",
        "year": "2003"
    },
    "czmiel_adaquate_2003-1": {
        "author": "Czmiel, Alexander",
        "keywords": "t\\_Encoding, t\\_XML, goal\\_Enrichment",
        "language": "de",
        "month": "oct,",
        "rCite": "czmiel_adaquate_2003-1",
        "rType": "phdthesis",
        "school": "Köln",
        "title": "Adäquate {Markupsysteme für die digitale {Behandlung altägyptischer {Texte",
        "type": "Thesis ({MA level)",
        "url": "http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf",
        "year": "2003"
    },
    "davis_universal_nodate": {
        "author": "Davis, Martin",
        "file": "Davis - The Universal Computer.pdf:files/100/Davis - The Universal Computer.pdf:application/pdf",
        "language": "en",
        "pages": "232",
        "rCite": "davis_universal_nodate",
        "rType": "article",
        "title": "The {Universal {Computer"
    },
    "de_santis_crossing_2019": {
        "address": "Berlin, Boston",
        "author": "De Santis, Annamaria and Rossi, Irene",
        "doi": "10.1515/9783110607208",
        "file": "Full Text:files/475/De Santis and Rossi - 2019 - Crossing Experiences in Digital Epigraphy, From Pr.pdf:application/pdf",
        "isbn": "978-3-11-060719-2",
        "language": "ENGL",
        "publisher": "De Gruyter",
        "rCite": "de_santis_crossing_2019",
        "rType": "book",
        "title": "Crossing {Experiences in {Digital {Epigraphy {From {Practice to {Discipline",
        "url": "https://www.degruyter.com/viewbooktoc/product/506243#",
        "urldate": "2019-08-21",
        "year": "2019"
    },
    "devlin_bert_2019": {
        "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "file": "Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:files/65/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf",
        "journal": "arXiv:1810.04805 [cs]",
        "keywords": "Computer Science - Computation and Language",
        "language": "en",
        "month": "may,",
        "note": "arXiv: 1810.04805",
        "rCite": "devlin_bert_2019",
        "rType": "article",
        "shorttitle": "{BERT",
        "title": "{BERT: {Pre-training of {Deep {Bidirectional {Transformers for {Language {Understanding",
        "url": "http://arxiv.org/abs/1810.04805",
        "urldate": "2020-10-10",
        "year": "2019"
    },
    "drucker_is_2013": {
        "author": "Drucker, Johanna",
        "doi": "10.1080/01973762.2013.761106",
        "file": "Drucker - 2013 - Is There a “Digital” Art History.pdf:files/24/Drucker - 2013 - Is There a “Digital” Art History.pdf:application/pdf",
        "issn": "0197-3762, 1477-2809",
        "journal": "Visual Resources",
        "language": "en",
        "month": "jun,",
        "number": "1-2",
        "pages": "5--13",
        "rCite": "drucker_is_2013",
        "rType": "article",
        "title": "Is {There a “{Digital” {Art {History?",
        "url": "http://www.tandfonline.com/doi/abs/10.1080/01973762.2013.761106",
        "urldate": "2020-10-10",
        "volume": "29",
        "year": "2013"
    },
    "edmond_digital_2020": {
        "doi": "10.11647/obp.0192",
        "editor": "Edmond, Jennifer",
        "file": "Full Text:files/320/Edmond - 2020 - Digital Technology and the Practices of Humanities.pdf:application/pdf",
        "isbn": "978-1-78374-839-6 978-1-78374-840-2 978-1-78374-841-9 978-1-78374-842-6 978-1-78374-843-3 978-1-78374-844-0",
        "keywords": "meta\\_GiveOverview, act\\_Publishing, obj\\_Research, goal\\_Dissemination",
        "language": "en",
        "month": "feb,",
        "publisher": "Open Book Publishers",
        "rCite": "edmond_digital_2020",
        "rType": "book",
        "title": "Digital {Technology and the {Practices of {Humanities {Research",
        "url": "https://www.openbookpublishers.com/product/1108",
        "urldate": "2020-02-04",
        "year": "2020"
    },
    "ekstein_text_2019": {
        "address": "Cham",
        "doi": "10.1007/978-3-030-27947-9",
        "editor": "Ekštein, Kamil",
        "file": "Ekštein - 2019 - Text, Speech, and Dialogue 22nd International Con.pdf:files/98/Ekštein - 2019 - Text, Speech, and Dialogue 22nd International Con.pdf:application/pdf",
        "isbn": "978-3-030-27946-2 978-3-030-27947-9",
        "language": "en",
        "publisher": "Springer International Publishing",
        "rCite": "ekstein_text_2019",
        "rType": "book",
        "series": "Lecture {Notes in {Computer {Science",
        "shorttitle": "Text, {Speech and {Dialogue",
        "title": "Text, {Speech and {Dialogue: 22nd {International {Conference {TSD 2019, {Ljubljana {Slovenia {September 11–13, 2019, {Proceedings",
        "url": "http://link.springer.com/10.1007/978-3-030-27947-9",
        "urldate": "2020-10-10",
        "volume": "11697",
        "year": "2019"
    },
    "evans_computational_2014": {
        "abstract": "In this paper I introduce computational techniques to extend qualitative analysis into the study of large textual datasets. I demonstrate these techniques by using probabilistic topic modeling to analyze a broad sample of 14,952 documents published in major American newspapers from 1980 through 2012. I show how computational data mining techniques can identify and evaluate the significance of qualitatively distinct subjects of discussion across a wide range of public discourse. I also show how examining large textual datasets with computational methods can overcome methodological limitations of conventional qualitative methods, such as how to measure the impact of particular cases on broader discourse, how to validate substantive inferences from small samples of textual data, and how to determine if identified cases are part of a consistent temporal pattern.",
        "author": "Evans, Michael S.",
        "doi": "10.1371/journal.pone.0087908",
        "file": "Full Text:files/208/Evans - 2014 - A Computational Approach to Qualitative Analysis i.pdf:application/pdf",
        "journal": "PLoS ONE",
        "keywords": "AnalyzeStatistically, bigdata",
        "language": "en",
        "month": "feb,",
        "number": "2",
        "rCite": "evans_computational_2014",
        "rType": "article",
        "title": "A {Computational {Approach to {Qualitative {Analysis in {Large {Textual {Datasets",
        "url": "http://dx.doi.org/10.1371/journal.pone.0087908",
        "urldate": "2014-02-04",
        "volume": "9",
        "year": "2014"
    },
    "feinerer_introduction_2008": {
        "author": "Feinerer, Ingo",
        "journal": "Rnews",
        "keywords": "AnalyzeStatistically, bigdata",
        "language": "en",
        "number": "2",
        "pages": "19-22",
        "rCite": "feinerer_introduction_2008",
        "rType": "article",
        "title": "An {Introduction to {Text {Mining in {R",
        "url": "http://140.247.115.171/CRAN/doc/Rnews/Rnews_2008-2.pdf#page",
        "volume": "8",
        "year": "2008"
    },
    "fitzpatrick_commentpress_2007": {
        "author": "Fitzpatrick, Kathleen",
        "doi": "http://dx.doi.org/10.3998/3336451.0010.305",
        "file": "Submitted Version:files/474/Fitzpatrick - 2007 - CommentPress New (Social) Structures for New (Net.pdf:application/pdf",
        "journal": "Journal of Electronic Publishing",
        "language": "en",
        "number": "3",
        "rCite": "fitzpatrick_commentpress_2007",
        "rType": "article",
        "title": "{CommentPress: {New ({Social) {Structures for {New ({Networked) {Texts",
        "url": "http://quod.lib.umich.edu/cgi/t/text/text-idx?c",
        "volume": "10",
        "year": "2007"
    },
    "friedlander_asking_2009": {
        "address": "Washington",
        "author": "Friedlander, Amy",
        "booktitle": "Working {Together or {Apart: {Promoting the {Next {Generation of {Digital {Scholarship",
        "keywords": "meta\\_GiveOverview, obj\\_DigitalHumanities, meta\\_Advocating",
        "language": "en",
        "pages": "1--15",
        "publisher": "Council on Library and Information Resources (CLIR)",
        "rCite": "friedlander_asking_2009",
        "rType": "incollection",
        "title": "Asking {Questions and {Building a {Research {Agenda for {Digital {Scholarship",
        "url": "http://www.clir.org/activities/digitalscholar2/friedlander.pdf",
        "year": "2009"
    },
    "galina_introduccion_2007": {
        "address": "Mexico City",
        "author": "Galina, Isabel and Ordoñez, C.",
        "keywords": "obj\\_Text, goal\\_Enrichment, act\\_Editing",
        "language": "es",
        "publisher": "Dirección General de Publicaciones y Fomento Editorial, UNAM",
        "rCite": "galina_introduccion_2007",
        "rType": "book",
        "series": "Colección {Biblioteca del {Editor",
        "title": "Introducción a la edición digital",
        "url": "http://www.anatomiadelaedicion.com/wordpress/wp-content/uploads/2010/01/manual-de-edicion-digital-1.pdf",
        "year": "2007"
    },
    "gelman_lets_2002": {
        "abstract": "Statisticians recommend graphical displays but often use tables to present their own research results. Could graphs do better? We study the question by going through the tables in a recent issue of the Journal of the American Statistical Association. We show how it is possible to improve the presentation using graphs that actually take up less space than the original tables. We find a particularly effective tool to be multiple repeated line plots, with comparisons of interest connected by lines and separate comparisons isolated on different plots.",
        "author": "Gelman, Andrew and Pascarica, Cristian and Dodhia, Rahul",
        "journal": "The American Statistician",
        "language": "en",
        "number": "2",
        "pages": "121--130",
        "rCite": "gelman_lets_2002",
        "rType": "article",
        "title": "Lets practice what we preach: {Turning tables into graphs",
        "url": "http://www.stat.ncsu.edu/people/fuentes/courses/st810a/gelman.pdf",
        "volume": "56",
        "year": "2002"
    },
    "gnadt_faktoren_2017": {
        "abstract": "Der Impact von digitalen Forschungsinfrastrukturen und Tools in der geistes- und kulturwissenschaftlichen Forschung ist von zentralem Interesse, wenn es darum geht die Vorteile solcher Entwicklungen zu einen den ForscherInnen selbst, zum anderen aber auch den Förderern zu vermitteln. In diesem Artikel werden Impactfaktoren und Erfolgskriterien in einem Bewertungskatalog gesammelt und aus Sicht verschiedener Stakeholder, FachwissenschaftlerInnen, DiensteanbieterInnen, DiensteentwicklerInnen und Förderer beurteilt. Dazu werden durchgeführte Umfragen ausgewertet, Kriterien und Maßzahlen aus der Literatur zusammengetragen und die daraus resultierenden Faktoren und Kennzahlen nach eindeutigen Impact-Bereichen katalogisiert. Berücksichtigt werden neben den gängigen quantitativen Kennzahlen auch qualitative Aspekte, die den Besonderheiten der Geistes- und Kulturwissenschaften Rechnung tragen.",
        "address": "Göttingen",
        "author": "Gnadt, Timo and Schmitt, Viola E. and Stiller, Juliane and Thoden, Klaus",
        "institution": "DARIAH-DE",
        "keywords": "obj\\_Tools, obj\\_DigitalHumanities, obj\\_Infrastructures, obj\\_Humanities",
        "language": "de",
        "note": "00000",
        "number": "21",
        "pages": "49",
        "rCite": "gnadt_faktoren_2017",
        "rType": "techreport",
        "title": "Faktoren und {Kriterien für den {Impact von {DH-{Tools und {Infrastrukturen",
        "url": "http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2017-21.pdf",
        "year": "2017"
    },
    "gold_debates_2019": {
        "doi": "10.5749/j.ctvg251hk",
        "editor": "Gold, Matthew K. and Klein, Lauren F.",
        "file": "Gold and Klein - 2019 - Debates in the Digital Humanities 2019.pdf:files/77/Gold and Klein - 2019 - Debates in the Digital Humanities 2019.pdf:application/pdf",
        "isbn": "978-1-4529-6166-8 978-1-5179-0693-1",
        "language": "en",
        "month": "apr,",
        "publisher": "University of Minnesota Press",
        "rCite": "gold_debates_2019",
        "rType": "book",
        "title": "Debates in the {Digital {Humanities 2019",
        "url": "http://www.jstor.org/stable/10.5749/j.ctvg251hk",
        "urldate": "2020-10-10",
        "year": "2019"
    },
    "gorz_kognitive_2007": {
        "author": "Görz, Günther",
        "journal": "Historisches Forum",
        "keywords": "act\\_Annotating, goal\\_Enrichment, obj\\_Maps",
        "language": "de",
        "rCite": "gorz_kognitive_2007",
        "rType": "article",
        "shorttitle": "Kognitive {Karten des {Mittelalters",
        "title": "Kognitive {Karten des {Mittelalters: {Digitale {Erschließung mittelalterlicher {Weltkarten.",
        "url": "http://wwwdh.cs.fau.de/IMMD8/staff/Goerz/mappae2006jb.pdf",
        "volume": "10",
        "year": "2007"
    },
    "grant_data_2019": {
        "address": "Boca Raton",
        "author": "Grant, Robert",
        "file": "Grant - 2019 - Data visualization charts, maps, and interactive .pdf:files/5/Grant - 2019 - Data visualization charts, maps, and interactive .pdf:application/pdf",
        "isbn": "978-1-138-55359-0 978-1-138-70760-3",
        "keywords": "Information visualization",
        "language": "en",
        "publisher": "CRC Press, Taylor \\& Francis Group",
        "rCite": "grant_data_2019",
        "rType": "book",
        "series": "{ASA-{CRC series on statistical reasoning in science and society",
        "shorttitle": "Data visualization",
        "title": "Data visualization: charts, maps, and interactive graphics",
        "year": "2019"
    },
    "haaf_measuring_2013": {
        "abstract": "Among mass digitization methods, double-keying is considered to be the one with the lowest error rate. This method requires two independent transcriptions of a text by two different operators. It is particularly well suited to historical texts, which often exhibit deficiencies like poor master copies or other difficulties such as spelling variation or complex text structures.            Providers of data entry services using the double-keying method generally advertise very high accuracy rates (around 99.95\\% to 99.98\\%). These advertised percentages are generally estimated on the basis of small samples, and little if anything is said about either the actual amount of text or the text genres which have been proofread, about error types, proofreaders, etc. In order to obtain significant data on this problem it is necessary to analyze a large amount of text representing a balanced sample of different text types, to distinguish the structural XML/TEI level from the typographical level, and to differentiate between various types of errors which may originate from different sources and may not be equally severe.         This paper presents an extensive and complex approach to the analysis and correction of double-keying errors which has been applied by the DFG-funded project \"Deutsches Textarchiv\" (German Text Archive, hereafter DTA) in order to evaluate and preferably to increase the transcription and annotation accuracy of double-keyed DTA texts. Statistical analyses of the results gained from proofreading a large quantity of text are presented, which verify the common accuracy rates for the double-keying method.",
        "author": "Haaf, Susanne and Wiegand, Frank and Geyken, Alexander",
        "collaborator": "Jannidis, Fotis and Rehbein, Malte and Romary, Laurent",
        "copyright": "TEI Consortium 2013 (Creative Commons Attribution-NoDerivs 3.0 Unported License)",
        "doi": "10.4000/jtei.739",
        "file": "Full Text:files/352/Haaf et al. - 2013 - Measuring the Correctness of Double-Keying Error .pdf:application/pdf",
        "issn": "2162-5603",
        "journal": "Journal of the Text Encoding Initiative",
        "keywords": "act\\_DataRecognition, bigdata{\\textasciitilde obj\\_Text",
        "language": "en",
        "month": "mar,",
        "number": "Issue 4",
        "rCite": "haaf_measuring_2013",
        "rType": "article",
        "shorttitle": "Measuring the {Correctness of {Double-{Keying",
        "title": "Measuring the {Correctness of {Double-{Keying: {Error {Classification and {Quality {Control in a {Large {Corpus of {TEI-{Annotated {Historical {Text",
        "url": "http://jtei.revues.org/739",
        "urldate": "2013-04-01",
        "year": "2013"
    },
    "hakinson_interchange_2010": {
        "address": "Utrecht, Netherlands",
        "author": "Hakinson, A. and Pugin, Laurent and Fujinaga, I.",
        "booktitle": "In {Proceedings of the 11th {International {Society for {Music {Information {Retrieval {Conference ({ISMIR 2010)",
        "keywords": "act\\_DataRecognition, obj\\_SheetMusic",
        "language": "en",
        "pages": "51--56",
        "rCite": "hakinson_interchange_2010",
        "rType": "inproceedings",
        "title": "An {Interchange {Format for {Optical {Music {Recognition {Applications",
        "url": "http://ismir2010.ismir.net/proceedings/ismir2010-11.pdf",
        "year": "2010"
    },
    "hermann_teaching_2015": {
        "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we deﬁne a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
        "annote": "Comment: Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 14 pages, 13 figures",
        "author": "Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil",
        "file": "Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:files/42/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf",
        "journal": "arXiv:1506.03340 [cs]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing",
        "language": "en",
        "month": "nov,",
        "note": "arXiv: 1506.03340",
        "rCite": "hermann_teaching_2015",
        "rType": "article",
        "title": "Teaching {Machines to {Read and {Comprehend",
        "url": "http://arxiv.org/abs/1506.03340",
        "urldate": "2020-10-10",
        "year": "2015"
    },
    "heuser_quantitative_2012": {
        "abstract": "The nineteenth century in Britain saw tumultuous changes that reshaped the fabric of society and altered the course of modernization. It also saw the rise of the novel to the height of its cultural power as the most important literary form of the period. This paper reports on a long-term experiment in tracing such macroscopic changes in the novel during this crucial period. Specifically, we present findings on two interrelated transformations in novelistic language that reveal a systemic concretization in language and fundamental change in the social spaces of the novel. We show how these shifts have consequences for setting, characterization, and narration as well as implications for the responsiveness of the novel to the dramatic changes in British society. This paper has a second strand as well. This project was simultaneously an experiment in developing quantitative and computational methods for tracing changes in literary language. We wanted to see how far quantifiable features such as word usage could be pushed toward the investigation of literary history. Could we leverage quantitative methods in ways that respect the nuance and complexity we value in the humanities? To this end, we present a second set of results, the techniques and methodological lessons gained in the course of designing and running this project.",
        "address": "Standford CA",
        "author": "Heuser, Ryan and Le-Khac, Long",
        "institution": "Literary Lab, Stanford University",
        "keywords": "bigdata",
        "language": "en",
        "month": "may,",
        "rCite": "heuser_quantitative_2012",
        "rType": "techreport",
        "title": "A {Quantitative {Literary {History of 2,958 {Nineteenth-{Century {British {Novels: {The {Semantic {Cohort {Method",
        "url": "http://litlab.stanford.edu/LiteraryLabPamphlet4.pdf",
        "year": "2012"
    },
    "jacomy_forceatlas2_2014": {
        "abstract": "Gephi is a network visualization software used in various disciplines (social network analysis, biology, genomics…). One of its key features is the ability to display the spatialization process, aiming at transforming the network into a map, and ForceAtlas2 is its default layout algorithm. The latter is developed by the Gephi team as an all-around solution to Gephi users’ typical networks (scale-free, 10 to 10,000 nodes). We present here for the first time its functioning and settings. ForceAtlas2 is a force-directed layout close to other algorithms used for network spatialization. We do not claim a theoretical advance but an attempt to integrate different techniques such as the Barnes Hut simulation, degree-dependent repulsive force, and local and global adaptive temperatures. It is designed for the Gephi user experience (it is a continuous algorithm), and we explain which constraints it implies. The algorithm benefits from much feedback and is developed in order to provide many possibilities through its settings. We lay out its complete functioning for the users who need a precise understanding of its behaviour, from the formulas to graphic illustration of the result. We propose a benchmark for our compromise between performance and quality. We also explain why we integrated its various features and discuss our design choices.",
        "author": "Jacomy, Mathieu and Venturini, Tommaso and Heymann, Sebastien and Bastian, Mathieu",
        "doi": "10.1371/journal.pone.0098679",
        "file": "Full Text:files/435/Jacomy et al. - 2014 - ForceAtlas2, a Continuous Graph Layout Algorithm f.pdf:application/pdf",
        "journal": "PLoS ONE",
        "keywords": "act\\_Visualizing, goal\\_Interpretation, obj\\_Data",
        "language": "en",
        "month": "jun,",
        "number": "6",
        "pages": "98679",
        "rCite": "jacomy_forceatlas2_2014",
        "rType": "article",
        "title": "{ForceAtlas2 a {Continuous {Graph {Layout {Algorithm for {Handy {Network {Visualization {Designed for the {Gephi {Software",
        "url": "http://dx.doi.org/10.1371/journal.pone.0098679",
        "urldate": "2014-06-18",
        "volume": "9",
        "year": "2014"
    },
    "juola_prototype_2006": {
        "abstract": "Despite a century of research, statistical and computational methods for authorship attribution are neither reliable, well-regarded, widely used, or well-understood. This article presents a survey of the current state of the art as well as a framework for uniform and unified development of a tool to apply the state of the art, despite the wide variety of methods and techniques used. The usefulness of the framework is confirmed by the development of a tool using that framework that can be applied to authorship analysis by researchers without a computing specialization. Using this tool, it may be possible both to expand the pool of available researchers as well as to enhance the quality of the overall solutions [for example, by incorporating improved algorithms as discovered through empirical analysis (Juola, P. (2004a). Ad-hoc Authorship Attribution Competition. In Proceedings 2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities (ALLC/ACH 2004), Göteborg, Sweden)].",
        "author": "Juola, Patrick and Sofko, John and Brennan, Patrick",
        "doi": "10.1093/llc/fql019",
        "file": "Submitted Version:files/210/Juola et al. - 2006 - A Prototype for Authorship Attribution Studies.pdf:application/pdf",
        "journal": "Literary and Linguistic Computing",
        "keywords": "AnalyzeStatistically, meta\\_Theorizing, t\\_Stylometry",
        "language": "en",
        "month": "jun,",
        "number": "2",
        "pages": "169 --178",
        "rCite": "juola_prototype_2006",
        "rType": "article",
        "title": "A {Prototype for {Authorship {Attribution {Studies",
        "url": "http://llc.oxfordjournals.org/content/21/2/169.abstract",
        "urldate": "2011-12-14",
        "volume": "21",
        "year": "2006"
    },
    "kazama_exploiting_2007": {
        "abstract": "We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER.",
        "author": "Kazama, Jun'ichi and Torisawa, Kentaro",
        "keywords": "t\\_NamedEntityRecognition",
        "language": "en",
        "pages": "698--707",
        "rCite": "kazama_exploiting_2007",
        "rType": "inproceedings",
        "title": "Exploiting {Wikipedia as {External {Knowledge for {Named {Entity {Recognition",
        "url": "http://www.aclweb.org/anthology-new/D/D07/D07-1073.pdf",
        "urldate": "2013-05-06",
        "year": "2007"
    },
    "kelleher_changing_2011": {
        "address": "Young Researchers Forum ESF Humanities, June 2011, Maynooth, Ireland.",
        "author": "Kelleher, Margaret and Hoogland, Eva",
        "institution": "European Science Foundation (ESF)",
        "keywords": "goal\\_Dissemination, meta\\_Advocating, obj\\_ResearchResults",
        "language": "en",
        "rCite": "kelleher_changing_2011",
        "rType": "techreport",
        "title": "Changing {Publication {Practices in the {Humanities.",
        "url": "http://www.esf.org/fileadmin/Public_documents/Publications/Changing_Publication_Cultures_Humanities.pdf",
        "year": "2011"
    },
    "kepper_musikedition_2011": {
        "abstract": "Die Keimzelle der Musikwissenschaft als geisteswissenschaftlicher Disziplin liegt in den Bemühungen des 19. Jahrhunderts, die Werke herausragender Komponisten zu konservieren und einer breiteren Öffentlichkeit zu erschließen. In diesem Umfeld erschien im Jahr 1851 der erste Band der Bach-Gesamtausgabe, herausgegeben von der Leipziger Bachgesellschaft. Alle nachfolgenden Musiker-Ausgaben entwickelten sich auf dieser Basis und reizten die Möglichkeiten des Buchmediums in zunehmenden Maße aus. Seit etwa zehn Jahren wird versucht, das Potential digitaler Medien für die Musikphilologie zu erschließen. Ausgehend von der Geschichte musikwissenschaftlicher Ausgaben und einer kritischen Reflektion des bisher Geleisteten, weist dieser Band mögliche neue Perspektiven für zukünftige, dem neuen Medium angemessene Editionsformen auf.",
        "address": "Norderstedt",
        "author": "Kepper, Johannes",
        "isbn": "978-3-8448-0076-0",
        "keywords": "meta\\_Theorizing, t\\_Encoding, obj\\_SheetMusic, obj\\_Music, goal\\_Enrichment",
        "language": "de",
        "publisher": "Books on Demand",
        "rCite": "kepper_musikedition_2011",
        "rType": "book",
        "shorttitle": "Musikedition im {Zeichen neuer {Medien",
        "title": "Musikedition im {Zeichen neuer {Medien : historische {Entwicklung und gegenwärtige {Perspektiven musikalischer {Gesamtausgaben",
        "url": "http://ifb.bsz-bw.de/bsz36848601Xrez-1.pdf",
        "year": "2011"
    },
    "kidd_new_2019": {
        "abstract": "With New Eyes I See (WNEIS) was an immersive and itinerant digital heritage encounter exploring the exploitation of empathy made possible in such emergent formats. Located ‘in the wild’, and timed to coincide with the 2014 Centenary of the First World War, WNEIS transformed Cardiff’s civic centre as previously inaccessible stories and archival materials were projected onto, and playfully manipulated by, buildings and the natural environment. The research that underpinned the project unearthed a hitherto untold story about the experiences and fates of those who left their posts at Amgueddfa Cymru – National Museum Wales to go and fight in WW1. Focusing on the story of Botanist Cyril Mortimer Green, and moving between past and present, known and unknown, presence and absence, participants encountered a re-scripting and multiple layering of the cityscape, and an uneasy archaeology of the museological endeavour. WNEIS foregrounded opportunities for touching, listening and feeling; as such it was a multimodal form of investigation for participants. This article uses focus group materials to explore the intersecting themes of ‘embodiment’, ‘empathy’ and ‘silence’ that emerged in reflections. It reveals an audience ready for digital cultural heritage that embraces ambiguity in the examination and negotiation of meaning.",
        "author": "Kidd, Jenny",
        "doi": "10.1080/13527258.2017.1341946",
        "file": "Kidd - 2019 - With New Eyes I See  embodiment, empathy a.pdf:files/20/Kidd - 2019 - With New Eyes I See  embodiment, empathy a.pdf:application/pdf",
        "issn": "1352-7258, 1470-3610",
        "journal": "International Journal of Heritage Studies",
        "language": "en",
        "month": "jan,",
        "number": "1",
        "pages": "54--66",
        "rCite": "kidd_new_2019",
        "rType": "article",
        "shorttitle": "\\textit{{With {New {Eyes {I {See",
        "title": "\\textit{{With {New {Eyes {I {See : embodiment, empathy and silence in digital heritage interpretation",
        "url": "https://www.tandfonline.com/doi/full/10.1080/13527258.2017.1341946",
        "urldate": "2020-10-10",
        "volume": "25",
        "year": "2019"
    },
    "king_making_2000": {
        "abstract": "Social Scientists rarely take full advantage of the information available in their statistical results. As a consequence, they miss opportunities to present quantities that are of greatest substantive interest for their research and express the appropriate degree of certainty about these quantities. In this article, we offer an approach, built on the technique of statistical simulation, to extract the currently overlooked information from any statistical method and to interpret and present it in a reader-friendly manner. Using this technique requires some expertise, which we try to provide herein, but its application should make the results of quantitative articles more informative and transparent. To illustrate our recommendations, we replicate the results of several published works, showing in each case how the authors’ own conclusions can be expressed more sharply and informatively, and, without changing any data or statistical assumptions, how our approach reveals important new information about the research questions at hand. We also offer very easy-to-use Clarify software that implements our suggestions",
        "author": "King, Gary and Tomz, Michael",
        "journal": "American Journal of Political Science",
        "language": "en",
        "number": "2",
        "pages": "341--355",
        "rCite": "king_making_2000",
        "rType": "article",
        "title": "Making the {Most of {Statistical {Analyses: {Interpretation and {Presentation",
        "url": "http://gking.harvard.edu/files/making.pdf",
        "volume": "44",
        "year": "2000"
    },
    "klimpel_forschen_2015": {
        "abstract": "Unter dem Schlagwort Digital Humanities finden nicht nur Techniken der quantitativen Analyse vermehrt   Eingang in den Methodenkanon der Geistes- und Kulturwissenschaften, auch ein Umdenken in der Verwaltung von Zugriffs- und Nutzungsrechten wird   erforderlich. Ob die Einführung von Standards sowohl für Zugangsberechtigung zu Forschungsergebnissen als auch für die Verwendung von Forschungsdaten in Form von freien Lizenzen hierfür ein geeignetes Mittel darstellen, soll u. a. mit diesem Dokument eruiert werden. Nach der Einführung woran Rechte entstehen, widmen sich weitere Kapitel dem wissenschaftlichen Arbeiten auf Basis fremder Inhalte, dem wissenschaftlichen Arbeiten als Quelle eigener Rechte, den Rechten der Forschungsinstitution oder Universität, Open Access, sowie der grenzüberschreitenden Forschung und Haftungsfragen.",
        "address": "Göttingen",
        "author": "Klimpel, Paul and Weitzmann, John H.",
        "institution": "DARIAH-DE",
        "keywords": "obj\\_DigitalHumanities, obj\\_Humanities, Open Access",
        "language": "de",
        "note": "00000",
        "number": "12",
        "pages": "36",
        "rCite": "klimpel_forschen_2015",
        "rType": "techreport",
        "title": "Forschen in der digitalen {Welt. {Juristische {Handreichung für die {Geisteswissenschaften",
        "url": "http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2015-12.pdf",
        "year": "2015"
    },
    "kraker_case_2011": {
        "abstract": "In this paper, we make the case for an open science in technology enhanced learning (TEL). Open science means opening up the research process by making all of its outcomes, and the way in which these outcomes were achieved, publicly available on the World Wide Web. In our vision, the adoption of open science instruments provides a set of solid and sustainable ways to connect the disjoint communities in TEL. Furthermore, we envision that researchers in TEL would be able to reproduce the results from any paper using the instruments of open science. Therefore, we introduce the concept of open methodology, which stands for sharing the methodological details of the evaluation provided, and the tools used for data collection and analysis. We discuss the potential benefits, but also the issues of an open science, and conclude with a set of recommendations for implementing open science in TEL.",
        "author": "Kraker, Peter and Leony, Derick and Reinhardt, Wolfgang and Gü, N.A. and Beham, nter",
        "doi": "10.1504/IJTEL.2011.045454",
        "file": "Kraker et al. - 2011 - The case for an open science in technology enhance.pdf:files/28/Kraker et al. - 2011 - The case for an open science in technology enhance.pdf:application/pdf",
        "issn": "1753-5255, 1753-5263",
        "journal": "International Journal of Technology Enhanced Learning",
        "language": "en",
        "number": "6",
        "pages": "643",
        "rCite": "kraker_case_2011",
        "rType": "article",
        "title": "The case for an open science in technology enhanced learning",
        "url": "http://www.inderscience.com/link.php?id",
        "urldate": "2020-10-10",
        "volume": "3",
        "year": "2011"
    },
    "labbe_tool_2006": {
        "abstract": "How to measure proximities and oppositions in large text corpora? Intertextual distance provides a simple and interesting solution. Its properties make it a good tool for text classification, and especially for tree-analysis which is fully presented and discussed here. In order to measure the quality of this classification, two indices are proposed. The method presented provides an accurate tool for literary studies—as is demonstrated by applying it to two areas of French literature, Racine's tragedies and an authorship attribution experiment.",
        "author": "Labbé, Cyril and Labbé, Dominique",
        "doi": "10.1093/llc/fqi063",
        "file": "Submitted Version:files/212/Labbé and Labbé - 2006 - A Tool for Literary Studies Intertextual Distance.pdf:application/pdf",
        "journal": "Literary and Linguistic Computing",
        "keywords": "AnalyzeStatistically, bigdata, t\\_Stylometry",
        "language": "en",
        "number": "3",
        "pages": "311 --326",
        "rCite": "labbe_tool_2006",
        "rType": "article",
        "shorttitle": "A {Tool for {Literary {Studies",
        "title": "A {Tool for {Literary {Studies: {Intertextual {Distance and {Tree {Classification",
        "url": "http://llc.oxfordjournals.org/content/21/3/311.abstract",
        "urldate": "2011-10-05",
        "volume": "21",
        "year": "2006"
    },
    "leroux_notion_2012": {
        "abstract": "We want to highlight the central role played by the notion of model in philosophy of science. Having first underlined the intricate tie existing between models and theories in science, we distinguish, in the philosophical literature, the notion of model as general conception from the structural notion of model currently encountered. We present in informal fashion and critically assess the main approaches adopted by epistemological analysis in its intent to characterize models generally associated with scientific theories. Lastly, and with a view to illustrating the centrality of the notion of model, we show how the current debate over scientific realism essentially hinges on contrasting ways to construe of models.",
        "author": "Leroux, Jean",
        "doi": "10.7202/1013054ar",
        "file": "Full Text:files/436/Leroux - 2012 - La notion de modèle en philosophie des sciences.pdf:application/pdf",
        "issn": "1712-8307, 1918-7475",
        "journal": "Nouvelles perspectives en sciences sociales",
        "keywords": "obj\\_Research, goal\\_Interpretation, act\\_Modeling",
        "language": "fr",
        "number": "2",
        "pages": "49",
        "rCite": "leroux_notion_2012",
        "rType": "article",
        "title": "La notion de modèle en philosophie des sciences",
        "url": "http://www.erudit.org/revue/npss/2012/v7/n2/1013054ar.html?vue",
        "urldate": "2014-09-02",
        "volume": "7",
        "year": "2012"
    },
    "masuzzo_you_2017": {
        "abstract": "The internet era, large-scale computing and storage resources, mobile devices, social media, and their high uptake among different groups of people, have all deeply changed the way knowledge is created, communicated, and further deployed. These advances have enabled a radical transformation of the practice of science, which is now more open, more global and collaborative, and closer to society than ever. Open science has therefore become an increasingly important topic. Moreover, as open science is actively pursued by several high-profile funders and institutions, it has fast become a crucial matter to all researchers. However, because this widespread interest in open science has emerged relatively recently, its definition and implementation are constantly shifting and evolving, sometimes leaving researchers in doubt about how to adopt open science, and which are the best practices to follow.",
        "author": "Masuzzo, Paola and Martens, Lennart",
        "doi": "10.7287/peerj.preprints.2689v1",
        "file": "Masuzzo and Martens - 2017 - Do you speak open science Resources and tips to l.pdf:files/30/Masuzzo and Martens - 2017 - Do you speak open science Resources and tips to l.pdf:application/pdf",
        "institution": "PeerJ Preprints",
        "language": "en",
        "month": "jan,",
        "rCite": "masuzzo_you_2017",
        "rType": "techreport",
        "shorttitle": "Do you speak open science?",
        "title": "Do you speak open science? {Resources and tips to learn the language",
        "type": "preprint",
        "url": "https://peerj.com/preprints/2689v1",
        "urldate": "2020-10-10",
        "year": "2017"
    },
    "mccarty_humanities_2005": {
        "address": "Basingstoke \\& New York",
        "author": "McCarty, Willard",
        "isbn": "978-1-4039-3504-5",
        "keywords": "meta\\_GiveOverview, obj\\_DigitalHumanities, *****, act\\_Conceptualizing, obj\\_Methods, act\\_Modeling",
        "language": "en",
        "publisher": "Palgrave Macmillan",
        "rCite": "mccarty_humanities_2005",
        "rType": "book",
        "title": "Humanities computing",
        "url": "http://www.mccarty.org.uk/essays/McCarty,%20Humanities%20computing.pdf",
        "year": "2005"
    },
    "mccarty_residue_2012": {
        "address": "Köln",
        "author": "McCarty, Willard",
        "keywords": "obj\\_AnyObject, act\\_Modeling",
        "language": "en",
        "month": "apr,",
        "pages": "23",
        "rCite": "mccarty_residue_2012",
        "rType": "inproceedings",
        "title": "The residue of uniqueness",
        "url": "http://www.cceh.uni-koeln.de/files/McCarty.pdf",
        "year": "2012"
    },
    "mckinney_python_nodate": {
        "author": "McKinney, Wes",
        "file": "McKinney - Python for Data Analysis.pdf:files/3/McKinney - Python for Data Analysis.pdf:application/pdf",
        "language": "en",
        "pages": "541",
        "rCite": "mckinney_python_nodate",
        "rType": "article",
        "title": "Python for {Data {Analysis"
    },
    "mikolov_distributed_2013": {
        "abstract": "The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.",
        "author": "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
        "file": "Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:files/32/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf",
        "journal": "arXiv:1310.4546 [cs, stat]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",
        "language": "en",
        "month": "oct,",
        "note": "arXiv: 1310.4546",
        "rCite": "mikolov_distributed_2013",
        "rType": "article",
        "title": "Distributed {Representations of {Words and {Phrases and their {Compositionality",
        "url": "http://arxiv.org/abs/1310.4546",
        "urldate": "2020-10-10",
        "year": "2013"
    },
    "mnih_scalable_nodate": {
        "abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",
        "author": "Mnih, Andriy and Hinton, Geoffrey E",
        "file": "Mnih and Hinton - A Scalable Hierarchical Distributed Language Model.pdf:files/83/Mnih and Hinton - A Scalable Hierarchical Distributed Language Model.pdf:application/pdf",
        "language": "en",
        "pages": "8",
        "rCite": "mnih_scalable_nodate",
        "rType": "article",
        "title": "A {Scalable {Hierarchical {Distributed {Language {Model"
    },
    "nadeau_survey_2007": {
        "abstract": "This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.",
        "author": "Nadeau, David and Sekine, Satoshi",
        "doi": "10.1075/li.30.1.03nad",
        "file": "Submitted Version:files/211/Nadeau and Sekine - 2007 - A survey of named entity recognition and classific.pdf:application/pdf",
        "journal": "Lingvisticae Investigationes",
        "keywords": "*****, t\\_NamedEntityRecognition",
        "language": "en",
        "number": "1",
        "pages": "3--26",
        "rCite": "nadeau_survey_2007",
        "rType": "article",
        "title": "A survey of named entity recognition and classification",
        "url": "http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002?token",
        "volume": "30",
        "year": "2007"
    },
    "nadeau_survey_2007-1": {
        "abstract": "This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.",
        "author": "Nadeau, David and Sekine, Satoshi",
        "doi": "10.1075/li.30.1.03nad",
        "file": "Submitted Version:files/389/Nadeau and Sekine - 2007 - A survey of named entity recognition and classific.pdf:application/pdf",
        "journal": "Lingvisticae Investigationes",
        "keywords": "*****, t\\_NamedEntityRecognition",
        "language": "en",
        "number": "1",
        "pages": "3--26",
        "rCite": "nadeau_survey_2007-1",
        "rType": "article",
        "title": "A survey of named entity recognition and classification",
        "url": "http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002?token",
        "volume": "30",
        "year": "2007"
    },
    "noack_modularity_2009": {
        "abstract": "Two natural and widely used representations for the community structure of networks are clusterings, which partition the vertex set into disjoint subsets, and layouts, which assign the vertices to positions in a metric space. This paper unifies prominent characterizations of layout quality and clustering quality, by showing that energy models of pairwise attraction and repulsion subsume Newman and Girvan's modularity measure. Layouts with optimal energy are relaxations of, and are thus consistent with, clusterings with optimal modularity, which is of practical relevance because both representations are complementary and often used together.",
        "author": "Noack, Andreas",
        "doi": "10.1103/PhysRevE.79.026102",
        "file": "Submitted Version:files/437/Noack - 2009 - Modularity clustering is force-directed layout.pdf:application/pdf",
        "issn": "1539-3755, 1550-2376",
        "journal": "Physical Review E",
        "keywords": "act\\_Visualizing",
        "language": "en",
        "month": "feb,",
        "number": "2",
        "rCite": "noack_modularity_2009",
        "rType": "article",
        "title": "Modularity clustering is force-directed layout",
        "url": "http://link.aps.org/doi/10.1103/PhysRevE.79.026102",
        "urldate": "2013-04-05",
        "volume": "79",
        "year": "2009"
    },
    "noauthor_commission_2012": {
        "address": "Brussels",
        "institution": "European Commission",
        "language": "en",
        "month": "jul,",
        "rCite": "noauthor_commission_2012",
        "rType": "techreport",
        "title": "Commission {Recommendation on access to and preservation of scientific information",
        "url": "http://ec.europa.eu/research/science-society/document_library/pdf_06/recommendation-access-and-preservation-scientific-information_en.pdf",
        "year": "2012"
    },
    "pennington_glove_2014": {
        "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
        "address": "Doha, Qatar",
        "author": "Pennington, Jeffrey and Socher, Richard and Manning, Christopher",
        "booktitle": "Proceedings of the 2014 {Conference on {Empirical {Methods in {Natural {Language {Processing ({EMNLP)",
        "doi": "10.3115/v1/D14-1162",
        "file": "Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:files/84/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf",
        "language": "en",
        "pages": "1532--1543",
        "publisher": "Association for Computational Linguistics",
        "rCite": "pennington_glove_2014",
        "rType": "inproceedings",
        "shorttitle": "Glove",
        "title": "Glove: {Global {Vectors for {Word {Representation",
        "url": "http://aclweb.org/anthology/D14-1162",
        "urldate": "2020-10-10",
        "year": "2014"
    },
    "pugin_map_2007": {
        "address": "Vienna, Austria",
        "author": "Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.",
        "booktitle": "In {Proceedings of the 8th {International {Conference on {Music {Information {Retrieval ({ISMIR 2007)",
        "keywords": "act\\_DataRecognition, obj\\_SheetMusic",
        "language": "en",
        "pages": "513--16",
        "rCite": "pugin_map_2007",
        "rType": "inproceedings",
        "title": "{MAP {Adaptation to {Improve {Optical {Music {Recognition of {Early {Music {Documents {Using {Hidden {Markov {Models",
        "url": "http://www.aruspix.net/publications/pugin07map.pdf",
        "year": "2007"
    },
    "pugin_optical_2006": {
        "address": "Victoria, Canada",
        "author": "Pugin, Laurent",
        "keywords": "act\\_DataRecognition, obj\\_SheetMusic",
        "language": "en",
        "rCite": "pugin_optical_2006",
        "rType": "inproceedings",
        "title": "Optical {Music {Recognition of {Early {Typographic {Prints using {Hidden {Markov {Models",
        "url": "http://www.aruspix.net/publications/pugin06optical.pdf",
        "year": "2006"
    },
    "pugin_reducing_2007": {
        "address": "Budapest, Hungary",
        "author": "Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.",
        "booktitle": "In {Proceedings of the {European {Conference on {Digital {Libraries ({ECDL 2007)",
        "keywords": "obj\\_SheetMusic, goal\\_Capture",
        "language": "en",
        "pages": "471--74",
        "rCite": "pugin_reducing_2007",
        "rType": "inproceedings",
        "title": "Reducing {Costs for {Digitising {Early {Music with {Dynamic {Adaptation",
        "url": "http://www.aruspix.net/publications/pugin07reducing.pdf",
        "year": "2007"
    },
    "rebelo_optical_2012": {
        "abstract": "For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones.",
        "author": "Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R. S. and Guedes, Carlos and Cardoso, Jamie S.",
        "doi": "10.1007/s13735-012-0004-6",
        "file": "Full Text:files/353/Rebelo et al. - 2012 - Optical music recognition state-of-the-art and op.pdf:application/pdf",
        "keywords": "meta\\_GiveOverview, act\\_DataRecognition, obj\\_SheetMusic",
        "language": "en",
        "pages": "173--190",
        "rCite": "rebelo_optical_2012",
        "rType": "article",
        "series": "1",
        "title": "Optical music recognition: state-of-the-art and open issues",
        "url": "http://download.springer.com/static/pdf/527/art%253A10.1007%252Fs13735-012-0004-6.pdf?auth66",
        "year": "2012"
    },
    "reitz_hitchhikers_nodate": {
        "author": "Reitz, Kenneth and Schlusser, Tanya",
        "file": "Reitz and Schlusser - The Hitchhiker’s Guide To Python.pdf:files/7/Reitz and Schlusser - The Hitchhiker’s Guide To Python.pdf:application/pdf",
        "language": "en",
        "pages": "322",
        "rCite": "reitz_hitchhikers_nodate",
        "rType": "article",
        "title": "The {Hitchhiker’s {Guide {To {Python"
    },
    "ribeiro_why_2016": {
        "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.",
        "author": "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
        "file": "Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:files/67/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf",
        "journal": "arXiv:1602.04938 [cs, stat]",
        "keywords": "Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence",
        "language": "en",
        "month": "aug,",
        "note": "arXiv: 1602.04938",
        "rCite": "ribeiro_why_2016",
        "rType": "article",
        "shorttitle": "\"{Why {Should {I {Trust {You?",
        "title": "\"{Why {Should {I {Trust {You?\": {Explaining the {Predictions of {Any {Classifier",
        "url": "http://arxiv.org/abs/1602.04938",
        "urldate": "2020-10-10",
        "year": "2016"
    },
    "ridge_crowdsourcing_nodate": {
        "author": "Ridge, Mia",
        "file": "Ridge - Crowdsourcing Our Cultural Heritage.pdf:files/9/Ridge - Crowdsourcing Our Cultural Heritage.pdf:application/pdf",
        "language": "en",
        "pages": "16",
        "rCite": "ridge_crowdsourcing_nodate",
        "rType": "article",
        "title": "Crowdsourcing {Our {Cultural {Heritage"
    },
    "sap_social_2020": {
        "abstract": "Warning: this paper contains content that may be offensive or upsetting.",
        "annote": "Comment: ACL 2020 Camera Ready; Data available at http://tinyurl.com/social-bias-frames",
        "author": "Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin",
        "file": "Sap et al. - 2020 - Social Bias Frames Reasoning about Social and Pow.pdf:files/73/Sap et al. - 2020 - Social Bias Frames Reasoning about Social and Pow.pdf:application/pdf",
        "journal": "arXiv:1911.03891 [cs]",
        "keywords": "Computer Science - Computation and Language",
        "language": "en",
        "month": "apr,",
        "note": "arXiv: 1911.03891",
        "rCite": "sap_social_2020",
        "rType": "article",
        "shorttitle": "Social {Bias {Frames",
        "title": "Social {Bias {Frames: {Reasoning about {Social and {Power {Implications of {Language",
        "url": "http://arxiv.org/abs/1911.03891",
        "urldate": "2020-10-10",
        "year": "2020"
    },
    "schier_cid_1995": {
        "abstract": "Das Forschungsprojekt CID widmet sich dreierlei Aufgaben: Fundiert durch Webers Theorie der analytischen Erzählung wird CID zum einen die Bauform und das erzähltechnische Regelwerk des Detektivromans und seiner «Untergattungen» untersuchen. Zum anderen geht es um die Entwicklung und Erprobung eines leistungsfähigen und komfortablen Instrumentariums zur computergestützten empirischen Analyse epischer Texte, das den Qualitätsanforderungen der sozialwissenschaftlichen Inhaltsanalyse entspricht. Mit Hilfe dieser Instrumente sollen schließlich wissenschaftlich relevante und repräsentative Ergebnisse für die Literaturwissenschaft ermittelt werden: Die Arbeitsfelder, in die CID aufgeteilt ist, haben verschiedene konstitutive Elemente epischer Texte im allgemeinen und des Detektivromans im besonderen zum Gegenstand.",
        "editor": "Schier, Dagmar and Giersch, Malchus",
        "keywords": "AnalyzeQualitatively, goal\\_Interpretation, obj\\_Literature",
        "language": "de",
        "publisher": "Frankfurt am Main: Lang 1995",
        "rCite": "schier_cid_1995",
        "rType": "book",
        "title": "{CID. {Computergestützte {Interpretationen von {Detektivromanen",
        "url": "http://www.gbv.de/dms/goettingen/184540100.pdf",
        "year": "1995"
    },
    "schoch_quantitative_2013": {
        "abstract": "The workshop on Quantitative Text Analysis for Literary History was the first in a series of DARIAH-DE expert workshops and took place from November 22 to 23 at the University of Würzburg, Germany. It brought together experts in the computational analysis of collections of literary texts from France, Germany, Poland and the US. This report provides some context on the DARIAH expert workshops and inroduces the specific goals of the workshop reported on here. Then, it summarizes the major issues raised by the participants in their initial statements and debated in the ensuing discussions. Finally, it describes the key results from the workshop, which include advances int he areas of data, tools and methods for quantitative text analysis.",
        "address": "Göttingen",
        "author": "Schöch, Christof and Jannidis, Fotis",
        "institution": "DARIAH-DE",
        "keywords": "obj\\_DigitalHumanities, goal\\_Analysis, obj\\_Humanities",
        "language": "en",
        "note": "00001",
        "number": "2",
        "pages": "13",
        "rCite": "schoch_quantitative_2013",
        "rType": "techreport",
        "title": "Quantitative {Text {Analysis for {Literary {History – {Report on a {DARIAH-{DE {Expert {Workshop",
        "url": "http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-2.pdf",
        "year": "2013"
    },
    "schrader_xml-datenformat_2007": {
        "address": "Tübingen",
        "author": "Schräder, Gregor",
        "collaborator": "Morent, Stefan",
        "keywords": "t\\_Encoding, t\\_XML, obj\\_Music",
        "language": "de",
        "rCite": "schrader_xml-datenformat_2007",
        "rType": "phdthesis",
        "school": "Tübingen",
        "title": "Ein {XML-{Datenformat zur {Repräsentation kritischer {Musikedition unter besonderer {Berücksichtigung von {Neumennotation",
        "type": "{MA level",
        "url": "http://www.dimused.uni-tuebingen.de/downloads/studienarbeit.pdf",
        "year": "2007"
    },
    "schumacher_big_2016": {
        "abstract": "Dieses Working Paper beschreibt die Inhalte, Darstellungsformen und die medialen Umsetzungsmöglichkeiten einer Lehrmittelsammlung zum Thema „Big Data Methodik in den Geistes- und Kulturwissenschaften”. Zudem wird ein   Disseminationskonzept entwickelt, das aufzeigt, auf welche Weise Inhalte, Themen und Instrumente dieses transdisziplinären Bereichs in den jeweiligen Fachdisziplinen vermittelt werden können. Das hier   vorgestellte Konzept einer Lehr- und Lernmittelsammlung ist auf die Nutzung und Anwendung von Big Data Technologien und Methoden für geistes- und kulturwissenschaftliche Forschungsfragen ausgerichtet.",
        "address": "Göttingen",
        "author": "Schumacher, Mareike and Held, Marcus and Falk, Claudia and Pernes, Stefan",
        "institution": "DARIAH-DE",
        "keywords": "obj\\_DigitalHumanities, obj\\_Methods, obj\\_Data, obj\\_Humanities",
        "language": "de",
        "note": "00000",
        "number": "15",
        "pages": "26",
        "rCite": "schumacher_big_2016",
        "rType": "techreport",
        "title": "Big {Data in den {Geisteswissenschaften: {Konzept für eine {Lehr- und {Lernmittelsammlung",
        "url": "http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-15.pdf",
        "year": "2016"
    },
    "sebastiani_machine_2002": {
        "author": "Sebastiani, Fabrizio",
        "doi": "10.1145/505282.505283",
        "file": "Sebastiani - 2002 - Machine learning in automated text categorization.pdf:files/96/Sebastiani - 2002 - Machine learning in automated text categorization.pdf:application/pdf",
        "issn": "0360-0300, 1557-7341",
        "journal": "ACM Computing Surveys",
        "language": "en",
        "month": "mar,",
        "number": "1",
        "pages": "1--47",
        "rCite": "sebastiani_machine_2002",
        "rType": "article",
        "title": "Machine learning in automated text categorization",
        "url": "https://dl.acm.org/doi/10.1145/505282.505283",
        "urldate": "2020-10-10",
        "volume": "34",
        "year": "2002"
    },
    "shneiderman_eyes_1996": {
        "abstract": "A useful starting point for designing advanced graphical user interfaces is the visual information seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. The paper offers a task by data type taxonomy with seven data types (one, two, three dimensional data, temporal and multi dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts)",
        "author": "Shneiderman, Ben",
        "booktitle": "Proceedings of {Visual {Languages 1996",
        "doi": "10.1109/VL.1996.545307",
        "file": "Submitted Version:files/438/Shneiderman - 1996 - The Eyes Have It A Task by Data Type Taxonomy for.pdf:application/pdf",
        "language": "en",
        "pages": "336 -- 343",
        "rCite": "shneiderman_eyes_1996",
        "rType": "inproceedings",
        "title": "The {Eyes {Have {It: {A {Task by {Data {Type {Taxonomy for {Information {Visualizations",
        "url": "http://www.cs.ubc.ca/~tmm/courses/old533/readings/shneiderman96eyes.pdf",
        "year": "1996"
    },
    "stapelfeldt_islandora_2013": {
        "abstract": "Islandora is an open-source software framework developed since 2006 by the University of Prince Edward Island's Robertson Library. The Islandora framework is designed to ease the management of security and workflow for digital assets, and to help implementers create custom interfaces for display, search, and discovery. Turnkey options are provided via tools and modules (\"solution packs\") designed to support the work of a particular knowledge domain (such as chemistry), a particular content type (such as a digitized newspaper), or a particular task (such as TEI encoding). While it does not yet have native support for TEI, Islandora provides a promising basis on which digital humanities scholars could manage the creation, editing, validation, display, and comparison of TEI-encoded text. UPEI's IslandLives project, with its forthcoming solution pack, provides insight into how an Islandora version 6 installation can support OCR text extraction, automatic structural/semantic encoding of text, and web-based TEI editing and display functions for site administrators. This article introduces the Islandora framework and its suitability for TEI, describes the IslandLives approach in detail, and briefly discusses recent work and future directions for TEI work in Islandora. The authors hope that interested readers may help contribute to the expansion of TEI-related services and features available to be used with Islandora.",
        "author": "Stapelfeldt, Kirsta and Moses, Donald",
        "collaborator": "Blanke, Tobias and Romary, Laurent",
        "copyright": "TEI Consortium 2013 (Creative Commons Attribution-NoDerivs 3.0 Unported License)",
        "doi": "10.4000/jtei.790",
        "file": "Full Text:files/391/Stapelfeldt and Moses - 2013 - Islandora and TEI Current and Emerging Applicatio.pdf:application/pdf",
        "issn": "2162-5603",
        "journal": "Journal of the Text Encoding Initiative",
        "keywords": "goal\\_Dissemination",
        "language": "en",
        "month": "jun,",
        "number": "Issue 5",
        "rCite": "stapelfeldt_islandora_2013",
        "rType": "article",
        "shorttitle": "Islandora and {TEI",
        "title": "Islandora and {TEI: {Current and {Emerging {Applications/{Approaches",
        "url": "http://jtei.revues.org/790",
        "urldate": "2013-10-17",
        "year": "2013"
    },
    "stewart_charles_2003": {
        "abstract": "This study is a test case in the use of stylometric techniques to provide an entrance into questions of literary criticism and interpretation. The study applies multivariate analysis to two texts of Charles Brockden Brown, sometimes considered the first professional writer in the United States. Both a scatter graph of a principal components analysis and a cluster analysis show that individual chapters from each of two novels (Wieland and Carwin) group together, except for three chapters of Wieland that cluster with the Carwin chapters. One chapter of Wieland that clusters with the Carwin chapters is narrated by the same character who narrates all of Carwin, thus providing statistical evidence that Brown has created a narrator with a distinctive voice. Accounting for the clustering of the other two chapters calls for a consideration of several of the more crucial and problematic interpretative issues in the novel, and suggests that quantitative analysis can indeed provide background and evidence for literary critical discussion and understanding.",
        "author": "Stewart, Larry L.",
        "doi": "10.1093/llc/18.2.129",
        "file": "Submitted Version:files/434/Stewart - 2003 - Charles Brockden Brown Quantitative Analysis and .pdf:application/pdf",
        "journal": "Literary and Linguistic Computing",
        "keywords": "AnalyzeStatistically, goal\\_Interpretation",
        "language": "en",
        "month": "jun,",
        "number": "2",
        "pages": "129 --138",
        "rCite": "stewart_charles_2003",
        "rType": "article",
        "shorttitle": "Charles {Brockden {Brown",
        "title": "Charles {Brockden {Brown: {Quantitative {Analysis and {Literary {Interpretation",
        "url": "http://llc.oxfordjournals.org/content/18/2/129.abstract",
        "urldate": "2011-06-03",
        "volume": "18",
        "year": "2003"
    },
    "stokes_digital_2015": {
        "author": "Stokes, Peter A.",
        "doi": "10.3389/fdigh.2015.00005",
        "file": "Stokes - 2015 - Digital Approaches to Paleography and Book History.pdf:files/22/Stokes - 2015 - Digital Approaches to Paleography and Book History.pdf:application/pdf",
        "issn": "2297-2668",
        "journal": "Frontiers in Digital Humanities",
        "language": "en",
        "month": "oct,",
        "rCite": "stokes_digital_2015",
        "rType": "article",
        "shorttitle": "Digital {Approaches to {Paleography and {Book {History",
        "title": "Digital {Approaches to {Paleography and {Book {History: {Some {Challenges {Present and {Future",
        "url": "http://journal.frontiersin.org/Article/10.3389/fdigh.2015.00005/abstract",
        "urldate": "2020-10-10",
        "volume": "2",
        "year": "2015"
    },
    "svensson_three_2020": {
        "author": "Svensson, Patrik",
        "file": "Svensson - 2020 - Three Premises of Big Digital Humanities.pdf:files/79/Svensson - 2020 - Three Premises of Big Digital Humanities.pdf:application/pdf",
        "language": "en",
        "pages": "50",
        "rCite": "svensson_three_2020",
        "rType": "article",
        "title": "Three {Premises of {Big {Digital {Humanities",
        "year": "2020"
    },
    "tjong_kim_sang_introduction_2003": {
        "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.",
        "address": "Stroudsburg, PA, USA",
        "author": "Tjong Kim Sang, Erik F. and De Meulder, Fien",
        "booktitle": "Proceedings of the seventh conference on {Natural language learning at {HLT-{NAACL 2003 - {Volume 4",
        "doi": "10.3115/1119176.1119195",
        "file": "Full Text:files/390/Tjong Kim Sang and De Meulder - 2003 - Introduction to the CoNLL-2003 shared task langua.pdf:application/pdf",
        "keywords": "t\\_NamedEntityRecognition",
        "language": "en",
        "pages": "142--147",
        "publisher": "Association for Computational Linguistics",
        "rCite": "tjong_kim_sang_introduction_2003",
        "rType": "inproceedings",
        "series": "{CONLL '03",
        "shorttitle": "Introduction to the {CoNLL-2003 shared task",
        "title": "Introduction to the {CoNLL-2003 shared task: language-independent named entity recognition",
        "url": "http://dx.doi.org/10.3115/1119176.1119195",
        "urldate": "2013-05-06",
        "year": "2003"
    },
    "turchin_quantitative_2018": {
        "abstract": "Do human societies from around the world exhibit similarities in the way that they are structured, and show commonalities in the ways that they have evolved? These are long-standing questions that have proven difficult to answer. To test between competing hypotheses, we constructed a massive repository of historical and archaeological information known as “Seshat: Global History Databank.” We systematically coded data on 414 societies from 30 regions around the world spanning the last 10,000 years. We were able to capture information on 51 variables reflecting nine characteristics of human societies, such as social scale, economy, features of governance, and information systems. Our analyses revealed that these different characteristics show strong relationships with each other and that a single principal component captures around three-quarters of the observed variation. Furthermore, we found that different characteristics of social complexity are highly predictable across different world regions. These results suggest that key aspects of social organization are functionally related and do indeed coevolve in predictable ways. Our findings highlight the power of the sciences and humanities working together to rigorously test hypotheses about general rules that may have shaped human history.",
        "author": "Turchin, Peter and Currie, Thomas E. and Whitehouse, Harvey and François, Pieter and Feeney, Kevin and Mullins, Daniel and Hoyer, Daniel and Collins, Christina and Grohmann, Stephanie and Savage, Patrick and Mendel-Gleason, Gavin and Turner, Edward and Dupeyron, Agathe and Cioni, Enrico and Reddish, Jenny and Levine, Jill and Jordan, Greine and Brandl, Eva and Williams, Alice and Cesaretti, Rudolf and Krueger, Marta and Ceccarelli, Alessandro and Figliulo-Rosswurm, Joe and Tuan, Po-Ju and Peregrine, Peter and Marciniak, Arkadiusz and Preiser-Kapeller, Johannes and Kradin, Nikolay and Korotayev, Andrey and Palmisano, Alessio and Baker, David and Bidmead, Julye and Bol, Peter and Christian, David and Cook, Connie and Covey, Alan and Feinman, Gary and Júlíusson, Árni Daníel and Kristinsson, Axel and Miksic, John and Mostern, Ruth and Petrie, Cameron and Rudiak-Gould, Peter and ter Haar, Barend and Wallace, Vesna and Mair, Victor and Xie, Liye and Baines, John and Bridges, Elizabeth and Manning, Joseph and Lockhart, Bruce and Bogaard, Amy and Spencer, Charles",
        "doi": "10.1073/pnas.1708800115",
        "file": "Turchin et al. - 2018 - Quantitative historical analysis uncovers a single.pdf:files/14/Turchin et al. - 2018 - Quantitative historical analysis uncovers a single.pdf:application/pdf",
        "issn": "0027-8424, 1091-6490",
        "journal": "Proceedings of the National Academy of Sciences",
        "language": "en",
        "month": "jan,",
        "number": "2",
        "pages": "E144--E151",
        "rCite": "turchin_quantitative_2018",
        "rType": "article",
        "title": "Quantitative historical analysis uncovers a single dimension of complexity that structures global variation in human social organization",
        "url": "http://www.pnas.org/lookup/doi/10.1073/pnas.1708800115",
        "urldate": "2020-10-10",
        "volume": "115",
        "year": "2018"
    },
    "van_hyning_harnessing_2019": {
        "abstract": "Full text search is vital to research of many kinds, but automatically creating transcriptions of most handwritten materials and ornate print is not yet technologically achievable. Even companies that have invested heavily in Optical Character Recognition (OCR) and search optimization, such as Alphabet (Google's parent company), have yet to make (or make widely available) technology for parsing handwritten or ornate text. Crowdsourcing conducted by scholars and cultural heritage practitioners offers an opportunity for us to engage with a diverse public who are interested in transcribing historical, literary, and other documents, in order to advance human knowledge. Crowdsourcing is a great vehicle for engaging students of all walks of life, with primary sources. Virtual volunteers all around the world are eager to learn and contribute to the vast project of making the world's textual records more widely accessible, not only for search, but for those, such as blind and partially sighted people, who use screen readers.",
        "author": "Van Hyning, Victoria",
        "doi": "10.1111/lic3.12507",
        "file": "Van Hyning - 2019 - Harnessing crowdsourcing for scholarly and GLAM pu.pdf:files/11/Van Hyning - 2019 - Harnessing crowdsourcing for scholarly and GLAM pu.pdf:application/pdf",
        "issn": "1741-4113, 1741-4113",
        "journal": "Literature Compass",
        "language": "en",
        "month": "apr,",
        "number": "3-4",
        "pages": "e12507",
        "rCite": "van_hyning_harnessing_2019",
        "rType": "article",
        "title": "Harnessing crowdsourcing for scholarly and {GLAM purposes",
        "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/lic3.12507",
        "urldate": "2020-10-10",
        "volume": "16",
        "year": "2019"
    },
    "van_zundert_alfalab_2009": {
        "abstract": "This paper presents project 'Alfalab'. Alfalab is a collaborative frame work project of the Royal Netherlands Academy of Arts and Sciences (KNAW). It explores the success and fail factors for virtual research collaboration and supporting digital infrastructure in the Humanities. It does so by delivering a virtual research environment engineered through a virtual R\\&D collaborative and by drawing in use cases and feedback from Humanities researchers from two research fields: textual historical text research and historical GIS-application. The motivation for the project is found in a number of commonly stated factors that seem to be inhibiting general application of virtualized research infrastructure in the Humanities. The paper outlines the project's motivation, key characteristics and implementation. One of the pilot applications is described in greater detail.",
        "author": "van Zundert, J. and Zeldenrust, D. and Beaulieu, A.",
        "booktitle": "Fifth {IEEE {International {Conference on e-{Science 2009. e-{Science '09",
        "doi": "10.1109/e-Science.2009.8",
        "file": "Full Text:files/214/van Zundert et al. - 2009 - Alfalab Construction and Deconstruction of a Digi.pdf:application/pdf",
        "isbn": "978-0-7695-3877-8",
        "keywords": "obj\\_VREs",
        "language": "en",
        "month": "dec,",
        "pages": "1--5",
        "publisher": "IEEE",
        "rCite": "van_zundert_alfalab_2009",
        "rType": "inproceedings",
        "shorttitle": "Alfalab",
        "title": "Alfalab: {Construction and {Deconstruction of a {Digital {Humanities {Experiment",
        "year": "2009"
    },
    "vaswani_attention_2017": {
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "annote": "Comment: 15 pages, 5 figures",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia",
        "file": "Vaswani et al. - 2017 - Attention Is All You Need.pdf:files/59/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf",
        "journal": "arXiv:1706.03762 [cs]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning",
        "language": "en",
        "month": "dec,",
        "note": "arXiv: 1706.03762",
        "rCite": "vaswani_attention_2017",
        "rType": "article",
        "title": "Attention {Is {All {You {Need",
        "url": "http://arxiv.org/abs/1706.03762",
        "urldate": "2020-10-10",
        "year": "2017"
    },
    "vaswani_attention_2017-1": {
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
        "annote": "Comment: 15 pages, 5 figures",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia",
        "file": "Vaswani et al. - 2017 - Attention Is All You Need.pdf:files/102/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf",
        "journal": "arXiv:1706.03762 [cs]",
        "keywords": "Computer Science - Computation and Language, Computer Science - Machine Learning",
        "language": "en",
        "month": "dec,",
        "note": "arXiv: 1706.03762",
        "rCite": "vaswani_attention_2017-1",
        "rType": "article",
        "title": "Attention {Is {All {You {Need",
        "url": "http://arxiv.org/abs/1706.03762",
        "urldate": "2020-10-26",
        "year": "2017"
    },
    "waltman_unified_2010": {
        "abstract": "In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the VOS mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999-2008. (C) 2010 Elsevier Ltd. All rights reserved.",
        "author": "Waltman, Ludo and van Eck, Nees Jan and Noyons, Ed C.M.",
        "doi": "10.1016/j.joi.2010.07.002",
        "file": "Submitted Version:files/213/Waltman et al. - 2010 - A unified approach to mapping and clustering of bi.pdf:application/pdf",
        "issn": "17511577",
        "journal": "Journal of Informetrics",
        "keywords": "act\\_Visualizing, bigdata",
        "language": "en",
        "month": "oct,",
        "number": "4",
        "pages": "629--635",
        "rCite": "waltman_unified_2010",
        "rType": "article",
        "title": "A unified approach to mapping and clustering of bibliometric networks",
        "url": "http://linkinghub.elsevier.com/retrieve/pii/S1751157710000660",
        "urldate": "2013-04-05",
        "volume": "4",
        "year": "2010"
    },
    "waltman_unified_2010-1": {
        "abstract": "In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the VOS mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999-2008. (C) 2010 Elsevier Ltd. All rights reserved.",
        "author": "Waltman, Ludo and van Eck, Nees Jan and Noyons, Ed C.M.",
        "doi": "10.1016/j.joi.2010.07.002",
        "file": "Submitted Version:files/433/Waltman et al. - 2010 - A unified approach to mapping and clustering of bi.pdf:application/pdf",
        "issn": "17511577",
        "journal": "Journal of Informetrics",
        "keywords": "act\\_Visualizing, bigdata",
        "language": "en",
        "month": "oct,",
        "number": "4",
        "pages": "629--635",
        "rCite": "waltman_unified_2010-1",
        "rType": "article",
        "title": "A unified approach to mapping and clustering of bibliometric networks",
        "url": "http://linkinghub.elsevier.com/retrieve/pii/S1751157710000660",
        "urldate": "2013-04-05",
        "volume": "4",
        "year": "2010"
    },
    "wang_glue_2019": {
        "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.",
        "annote": "Comment: ICLR 2019; https://gluebenchmark.com/",
        "author": "Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.",
        "file": "Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:files/62/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf",
        "journal": "arXiv:1804.07461 [cs]",
        "keywords": "Computer Science - Computation and Language",
        "language": "en",
        "month": "feb,",
        "note": "arXiv: 1804.07461",
        "rCite": "wang_glue_2019",
        "rType": "article",
        "shorttitle": "{GLUE",
        "title": "{GLUE: {A {Multi-{Task {Benchmark and {Analysis {Platform for {Natural {Language {Understanding",
        "url": "http://arxiv.org/abs/1804.07461",
        "urldate": "2020-10-10",
        "year": "2019"
    },
    "windhager_visualization_2019": {
        "abstract": "After decades of digitization, large cultural heritage collections have emerged on the web, which contain massive stocks of content from galleries, libraries, archives, and museums. This increase in digital cultural heritage data promises new modes of analysis and increased levels of access for academic scholars and casual users alike. Going beyond the standard representations of search-centric and grid-based interfaces, a multitude of approaches has recently started to enable visual access to cultural collections, and to explore them as complex and comprehensive information spaces by the means of interactive visualizations. In contrast to conventional web interfaces, we witness a widening spectrum of innovative visualization types specially designed for rich collections from the cultural heritage sector. This new class of information visualizations gives rise to a notable diversity of interaction and representation techniques while lending currency and urgency to a discussion about principles such as serendipity, generosity, and criticality in connection with visualization design. With this survey, we review information visualization approaches to digital cultural heritage collections and reﬂect on the state of the art in techniques and design choices. We contextualize our survey with humanist perspectives on the ﬁeld and point out opportunities for future research.",
        "author": "Windhager, Florian and Federico, Paolo and Schreder, Gunther and Glinka, Katrin and Dork, Marian and Miksch, Silvia and Mayr, Eva",
        "doi": "10.1109/TVCG.2018.2830759",
        "file": "Windhager et al. - 2019 - Visualization of Cultural Heritage Collection Data.pdf:files/26/Windhager et al. - 2019 - Visualization of Cultural Heritage Collection Data.pdf:application/pdf",
        "issn": "1077-2626, 1941-0506, 2160-9306",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "language": "en",
        "month": "jun,",
        "number": "6",
        "pages": "2311--2330",
        "rCite": "windhager_visualization_2019",
        "rType": "article",
        "shorttitle": "Visualization of {Cultural {Heritage {Collection {Data",
        "title": "Visualization of {Cultural {Heritage {Collection {Data: {State of the {Art and {Future {Challenges",
        "url": "https://ieeexplore.ieee.org/document/8352050/",
        "urldate": "2020-10-10",
        "volume": "25",
        "year": "2019"
    }
}